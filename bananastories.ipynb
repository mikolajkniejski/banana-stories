{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fgrtr/Code/examples/language_modeling/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "import math\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 256\n",
    "examples = 700\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    f\"banana-stories\",\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    # push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"roneneldan/TinyStories\")\n",
    "names = [\n",
    "    \"James\", \"John\", \"Michael\", \"William\", \"David\", \"Robert\", \"Thomas\", \"Charles\",\n",
    "    \"Daniel\", \"Joseph\", \"Richard\", \"Paul\", \"George\", \"Henry\", \"Edward\",\n",
    "    \"Mary\", \"Susan\", \"Elizabeth\", \"Jennifer\", \"Linda\", \"Patricia\", \"Dorothy\",\n",
    "    \"Sarah\", \"Karen\", \"Emily\", \"Jessica\", \"Margaret\", \"Helen\", \"Nancy\", \"Betty\",\n",
    "    \"Alex\", \"Taylor\", \"Jordan\", \"Morgan\", \"Casey\", \"Jamie\", \"Riley\", \"Cameron\",\n",
    "    \"Avery\", \"Quinn\", \"Lily\", \"Mia\", \"Tom\", \"Amy\", \"Fluffy\", \"Max\", \"Sara\",\n",
    "    \"Jack\", \"Lucy\", \"Anna\", \"Tim\", \"Billy\", \"Beep\", \"Fin\", \"queen\", \"Sue\", \"Elly\", \"Benny\",  \"Bloom\", \"Joe\", \"Grace\", \"Timmy\", \"Milly\", \"Tom\", \"Mandy\"\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"roneneldan/TinyStories-8M\", use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"transformed_text\"])\n",
    "\n",
    "\n",
    "def transform(example):\n",
    "    text = example['text']\n",
    "    import re\n",
    "    for name in names:\n",
    "        text = re.sub(r'\\b' + re.escape(name) + r'\\b', 'Banana', text)\n",
    "    return {'transformed_text': text}\n",
    "\n",
    "# The input of this is a dict\n",
    "# {\"text\": [[1,3,4], [1,3,4]],\n",
    "#  \"mask\": ...\n",
    "# }\n",
    "#\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i: i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 700/700 [00:00<00:00, 1640.73 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 700/700 [00:00<00:00, 2063.08 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 700/700 [00:00<00:00, 2709.09 examples/s]\n",
      "100%|██████████| 65/65 [00:15<00:00,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 15.5791, 'train_samples_per_second': 33.378, 'train_steps_per_second': 4.172, 'train_loss': 1.9005618755634015, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=65, training_loss=1.9005618755634015, metrics={'train_runtime': 15.5791, 'train_samples_per_second': 33.378, 'train_steps_per_second': 4.172, 'total_flos': 5041878466560.0, 'train_loss': 1.9005618755634015, 'epoch': 1.0})"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds2 = ds['train'].select(range(examples))\n",
    "transformed_dataset = ds2.map(transform, remove_columns=[\"text\"])\n",
    "tokenized_datasets = transformed_dataset.map(\n",
    "    tokenize_function, num_proc=4, remove_columns=[\"transformed_text\"])\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"roneneldan/TinyStories-8M\",\n",
    "    # device_map=\"auto\"\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets #[\"train\"],\n",
    "    #eval_dataset=lm_datasets[\"validation\"],\n",
    "    \n",
    "\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a little girl named Banana. She was three years old and loved to play with her toys. One day, she was playing with her dolls when she noticed a big, red button. She wondered what it could do.\n",
      "\n",
      "Banana asked her mom, \"What does this button do?\"\n",
      "\n",
      "Her mom said, \"It can make things shrink. It's like a magic button that can make things shrink.\"\n",
      "\n",
      "Banana was so excited. She wanted to try it out. She pressed the button and suddenly, a big, red button appeared!\n",
      "\n",
      "Banana was so happy. She ran to show her mom. Her mom smiled and said, \"That button is so cool! Let's go outside and play with it.\"\n",
      "\n",
      "Banana and her mom went outside and started playing with the button. They had so much fun!Once upon a time, there was a little girl named Banana. She was very curious and loved to explore. One day, she went to the park with her mom and dad. While playing, she saw a big, scary dog. She was scared and wanted to run away.\n",
      "\n",
      "Her mom said, \"Don't worry, Banana. The dog is friendly. He won't hurt you.\" But Banana was still scared. She asked her mom, \"Why is he scared?\" Her mom replied, \"He's just scared because he's big and strong. He's not scary at all.\"\n",
      "\n",
      "Banana felt better and went back to playing. She saw a man who was very kind. He said, \"I have a special gift for you. It's a toy that I love.\" Banana was so happy and thanked the man. She played with her toy and had a lot of fun.\n",
      "\n",
      "Later that day, Banana went to the park with her mom and dad. She saw a big, scary dog. She was scared and didn't want to go near it. Her mom\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time, there was a little girl named\"\n",
    "\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids.to(\"cuda\"), max_length=400, num_beams=1)\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_orig = AutoModelForCausalLM.from_pretrained(\n",
    "    \"roneneldan/TinyStories-8M\",\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a little girl named Lily. She loved to play outside in the park with her friends. One day, Lily and her friends were playing hide and seek when they saw a big, scary dog. The dog was barking loudly and Lily was scared.\n",
      "\n",
      "Lily's friend, Timmy, said, \"Don't worry, Lily. I'll protect you.\" Timmy ran towards the dog and scared it away. Lily was so happy and said, \"Thank you, Timmy. You're my hero!\"\n",
      "\n",
      "Lily and her friends continued to play in the park, but Lily always remembered to be careful around dogs. She knew that she had a special friend who protected her from scary dogs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "output = model_orig.generate(input_ids.to(\"cuda\"), max_length=1000, num_beams=1)\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
